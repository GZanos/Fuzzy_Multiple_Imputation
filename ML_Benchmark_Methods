# ==================================================================================================
# Generalized Imputation Methods 
# ==================================================================================================
# WORKFLOW:
# 1. Separate training data into:
#    - Complete cases: rows where target is not NA
#    - Missing cases: rows where target is NA
# 2. Split complete cases into train/test 
# 3. Train each imputation method on Train set only 
# 4. Evaluate on Test set 
# 5. Retrain on all complete cases and predict the actual missing values
# 6. Compare against validation set as sanity check 
# ==================================================================================================

# USER CONFIGURATION
# ===================

DATASET_NAME <- "Maternal_Health"  # Change this to any of your dataset names
id_col <- "col_ID" # Change this to the ID column between train and validate datasets
target_col <- "Target" # Change this to the target column with missing values

# Train/Test split parameters
TEST_SPLIT <- 0.3  

# ==================================================================================================
# Setup and Data Loading
# ==================================================================================================

# --- Packages ---
req <- c("Amelia","missForest","VIM","softImpute","xgboost","Metrics","dplyr","reticulate","mice")
to_install <- setdiff(req, rownames(installed.packages()))
if(length(to_install)) install.packages(to_install, dependencies = TRUE)

library(Amelia)
library(missForest)
library(VIM)
library(softImpute)
library(xgboost)
library(Metrics)
library(dplyr)
library(reticulate)
library(mice)

# --- Configuration ---
set.seed(123)

# --- Load data ---
train_file <- paste0(DATASET_NAME, "_train.csv")
validate_file <- paste0(DATASET_NAME, "_validate.csv")

cat("Loading dataset:", DATASET_NAME, "\n")
cat("Train file:", train_file, "\n")
cat("Validate file:", validate_file, "\n")

# Check if files exist
if(!file.exists(train_file)) {
  stop("Training file not found: ", train_file)
}
if(!file.exists(validate_file)) {
  stop("Validation file not found: ", validate_file)
}

train <- read.csv(train_file, stringsAsFactors = FALSE)
validate <- read.csv(validate_file, stringsAsFactors = FALSE)

# Convert target to numeric if it's character
if(is.character(train[[target_col]])) {
  train[[target_col]][trimws(train[[target_col]]) == ""] <- NA
}
train[[target_col]] <- suppressWarnings(as.numeric(train[[target_col]]))

# --- Identify rows to impute ---
missing_ids <- train[[id_col]][is.na(train[[target_col]])]
cat("Found", length(missing_ids), "missing values to impute\n")

# Store validation data for final check (DO NOT USE FOR MODEL SELECTION)
validate_true_vals <- validate[validate[[id_col]] %in% missing_ids, c(id_col, target_col)]
colnames(validate_true_vals) <- c(id_col, "true_target")
cat("Validation set has", nrow(validate_true_vals), "corresponding values for final check\n")

# --- Ensure predictors are numeric ---
feature_cols <- setdiff(colnames(train), c(id_col, target_col))
cat("Feature columns:", length(feature_cols), "->", paste(head(feature_cols, 5), collapse = ", "), 
    if(length(feature_cols) > 5) "..." else "", "\n")

for (fc in feature_cols) {
  if(!is.numeric(train[[fc]])) {
    train[[fc]] <- suppressWarnings(as.numeric(train[[fc]]))
  }
}

# ==================================================================================================
# Separate Complete Cases from Missing Target Cases
# ==================================================================================================

cat("\n====================================\n")
cat("SEPARATING DATA\n")
cat("====================================\n")

# Identify rows with complete target values (for training)
complete_idx <- which(!is.na(train[[target_col]]))
cat("Complete cases (target not NA):", length(complete_idx), "\n")

# Identify rows with missing target values (to be predicted)
missing_idx <- which(is.na(train[[target_col]]))
cat("Missing cases (target is NA):", length(missing_idx), "\n")

if(length(complete_idx) < 20) {
  stop("Not enough complete cases for training. Need at least 20, found ", length(complete_idx))
}

# Create train/test split from ONLY the complete cases
set.seed(123)
n_test <- floor(length(complete_idx) * TEST_SPLIT)
test_idx <- sample(complete_idx, n_test)
train_idx <- setdiff(complete_idx, test_idx)

cat("\nTrain/Test Split (from complete cases only):\n")
cat("- Train set:", length(train_idx), "samples\n")
cat("- Test set:", n_test, "samples\n")
cat("- Will predict:", length(missing_idx), "missing values\n\n")

# ==================================================================================================
# Performance Metrics and Evaluation Functions
# ==================================================================================================

safe_mape <- function(actual, pred) {
  actual <- as.numeric(actual); pred <- as.numeric(pred)
  ok <- !is.na(actual) & !is.na(pred) & (actual != 0)
  if(!any(ok)) return(NA_real_)
  mean(abs((pred[ok] - actual[ok]) / actual[ok]))
}

compute_metrics <- function(actual, pred) {
  actual <- as.numeric(actual); pred <- as.numeric(pred)
  data.frame(
    MSE = mean((pred - actual)^2, na.rm = TRUE),
    MAE = mean(abs(pred - actual), na.rm = TRUE),
    RMSE = sqrt(mean((pred - actual)^2, na.rm = TRUE)),
    MAPE = safe_mape(actual, pred),
    check.names = FALSE
  )
}

# ==================================================================================================
# Imputation Method Implementations
# ==================================================================================================
# Each method trains on train_idx and predicts on pred_idx

impute_mean <- function(data, target_col, train_idx, pred_idx) {
  result <- rep(NA, nrow(data))
  result[train_idx] <- data[train_idx, target_col]
  mean_val <- mean(data[train_idx, target_col], na.rm = TRUE)
  result[pred_idx] <- mean_val
  return(result)
}

impute_median <- function(data, target_col, train_idx, pred_idx) {
  result <- rep(NA, nrow(data))
  result[train_idx] <- data[train_idx, target_col]
  median_val <- median(data[train_idx, target_col], na.rm = TRUE)
  result[pred_idx] <- median_val
  return(result)
}

impute_knn <- function(data, target_col, feature_cols, train_idx, pred_idx, k = 1) {
  result <- rep(NA, nrow(data))
  result[train_idx] <- data[train_idx, target_col]
  
  try({
    # Use only complete training data
    train_data <- data[train_idx, c(target_col, feature_cols)]
    
    # For each prediction point, find k nearest neighbors from training data
    for(i in pred_idx) {
      pred_features <- data[i, feature_cols]
      
      # Calculate distances to all training points
      distances <- sapply(train_idx, function(j) {
        sqrt(sum((data[j, feature_cols] - pred_features)^2, na.rm = TRUE))
      })
      
      # Get k nearest neighbors
      nearest <- train_idx[order(distances)[1:min(k, length(train_idx))]]
      
      # Average their target values
      result[i] <- mean(data[nearest, target_col], na.rm = TRUE)
    }
  }, silent = TRUE)
  
  # Fallback to mean
  if(any(is.na(result[pred_idx]))) {
    mean_val <- mean(data[train_idx, target_col], na.rm = TRUE)
    result[pred_idx][is.na(result[pred_idx])] <- mean_val
  }
  
  return(result)
}

impute_xgboost <- function(data, target_col, feature_cols, train_idx, pred_idx, 
                           depth = 1, rounds = 2, eta = 0.00001) {
  result <- rep(NA, nrow(data))
  result[train_idx] <- data[train_idx, target_col]
  
  try({
    # Prepare training data
    X_train <- as.matrix(data[train_idx, feature_cols])
    y_train <- data[train_idx, target_col]
    
    # Remove any rows with NA features
    complete_features <- complete.cases(X_train)
    X_train <- X_train[complete_features, , drop = FALSE]
    y_train <- y_train[complete_features]
    
    if(length(y_train) > 10) {
      dtrain <- xgb.DMatrix(data = X_train, label = y_train)
      
      params <- list(
        objective = "reg:squarederror",
        max_depth = depth,
        eta = eta,
        verbosity = 0
      )
      
      xgb_model <- xgb.train(params, dtrain, nrounds = rounds, verbose = 0)
      
      # Predict on pred_idx
      X_pred <- as.matrix(data[pred_idx, feature_cols])
      dpred <- xgb.DMatrix(data = X_pred)
      result[pred_idx] <- predict(xgb_model, dpred)
    }
  }, silent = TRUE)
  
  # Fallback to mean
  if(any(is.na(result[pred_idx]))) {
    mean_val <- mean(data[train_idx, target_col], na.rm = TRUE)
    result[pred_idx][is.na(result[pred_idx])] <- mean_val
  }
  
  return(result)
}

impute_mice <- function(data, target_col, feature_cols, train_idx, pred_idx,
                        method = "pmm", m = 5, maxit = 5) {
  result <- rep(NA, nrow(data))
  result[train_idx] <- data[train_idx, target_col]
  
  try({
    # Select top correlated features
    complete_train <- train_idx[complete.cases(data[train_idx, feature_cols])]
    if(length(complete_train) > 20) {
      cors <- sapply(feature_cols, function(x) {
        abs(cor(data[complete_train, target_col], data[complete_train, x], use = "complete.obs"))
      })
      cors[is.na(cors)] <- 0
      top_features <- names(sort(cors, decreasing = TRUE)[1:min(5, length(feature_cols))])
      
      # Create dataset with train + pred indices
      all_idx <- c(train_idx, pred_idx)
      mice_data <- data[all_idx, c(target_col, top_features)]
      
      # Mark which rows need imputation
      mice_data[match(pred_idx, all_idx), target_col] <- NA
      
      method_vec <- rep(method, ncol(mice_data))
      names(method_vec) <- colnames(mice_data)
      
      mice_result <- mice(mice_data, m = m, method = method_vec, 
                          maxit = maxit, printFlag = FALSE, seed = 123)
      
      completed_list <- complete(mice_result, action = "all")
      
      # Average predictions across imputations
      pred_rows <- match(pred_idx, all_idx)
      for(i in seq_along(pred_idx)) {
        values <- sapply(completed_list, function(df) df[pred_rows[i], target_col])
        result[pred_idx[i]] <- mean(values, na.rm = TRUE)
      }
    }
  }, silent = TRUE)
  
  # Fallback to mean
  if(any(is.na(result[pred_idx]))) {
    mean_val <- mean(data[train_idx, target_col], na.rm = TRUE)
    result[pred_idx][is.na(result[pred_idx])] <- mean_val
  }
  
  return(result)
}

impute_mvn <- function(data, target_col, feature_cols, train_idx, pred_idx) {
  result <- rep(NA, nrow(data))
  result[train_idx] <- data[train_idx, target_col]
  
  try({
    # Select top correlated features
    complete_train <- train_idx[complete.cases(data[train_idx, c(target_col, feature_cols)])]
    if(length(complete_train) > 20) {
      cors <- sapply(feature_cols, function(x) {
        abs(cor(data[complete_train, target_col], data[complete_train, x], use = "complete.obs"))
      })
      cors[is.na(cors)] <- 0
      top_features <- names(sort(cors, decreasing = TRUE)[1:min(3, length(feature_cols))])
      
      # Create dataset with train + pred indices
      all_idx <- c(train_idx, pred_idx)
      am_data <- data[all_idx, c(target_col, top_features)]
      
      # Mark which rows need imputation
      am_data[match(pred_idx, all_idx), target_col] <- NA
      
      a.out <- amelia(am_data, m = 1, ridge = 0.05, p2s = 0)
      
      if(!is.null(a.out$imputations) && length(a.out$imputations) > 0) {
        imputed_data <- a.out$imputations[[1]][[target_col]]
        pred_rows <- match(pred_idx, all_idx)
        result[pred_idx] <- imputed_data[pred_rows]
      }
    }
  }, silent = TRUE)
  
  # Fallback to mean
  if(any(is.na(result[pred_idx]))) {
    mean_val <- mean(data[train_idx, target_col], na.rm = TRUE)
    result[pred_idx][is.na(result[pred_idx])] <- mean_val
  }
  
  return(result)
}

impute_missforest <- function(data, target_col, feature_cols, train_idx, pred_idx,
                              ntree = 4, maxiter = 2) {
  result <- rep(NA, nrow(data))
  result[train_idx] <- data[train_idx, target_col]
  
  try({
    # Create dataset with train + pred indices
    all_idx <- c(train_idx, pred_idx)
    mf_data <- data[all_idx, c(target_col, feature_cols)]
    
    # Mark which rows need imputation
    mf_data[match(pred_idx, all_idx), target_col] <- NA
    
    mf <- missForest(mf_data, ntree = ntree, maxiter = maxiter, verbose = FALSE)
    
    imputed_data <- mf$ximp[[target_col]]
    pred_rows <- match(pred_idx, all_idx)
    result[pred_idx] <- imputed_data[pred_rows]
  }, silent = TRUE)
  
  # Fallback to mean
  if(any(is.na(result[pred_idx]))) {
    mean_val <- mean(data[train_idx, target_col], na.rm = TRUE)
    result[pred_idx][is.na(result[pred_idx])] <- mean_val
  }
  
  return(result)
}

# ==================================================================================================
# Define Model Configurations to Test
# ==================================================================================================

model_configs <- list(
  list(name = "Mean_imputation", func = impute_mean),
  list(name = "Median_imputation", func = impute_median),
  list(name = "KNN_k5", func = impute_knn, params = list(k = 5)),
  list(name = "XGBoost", func = impute_xgboost, params = list(depth = 3, rounds = 50, eta = 0.3)),
  list(name = "MICE_pmm", func = impute_mice, params = list(method = "pmm", m = 5, maxit = 5)),
  list(name = "MVN_Amelia", func = impute_mvn),
  list(name = "missForest", func = impute_missforest, params = list(ntree = 10, maxiter = 2))
)

# ==================================================================================================
# Evaluate All Models on Test Set
# ==================================================================================================

cat("\n====================================\n")
cat("EVALUATING MODELS ON TEST SET\n")
cat("====================================\n")

test_results <- data.frame()
test_results_file <- NULL

cat("Testing", length(model_configs), "imputation methods\n")
cat("Training on", length(train_idx), "samples\n")
cat("Testing on", length(test_idx), "samples\n\n")

# Test each model
for(model_config in model_configs) {
  model_name <- model_config$name
  
  cat("Testing:", model_name, "...")
  
  tryCatch({
    # Call imputation function with train and test indices
    if(model_name %in% c("Mean_imputation", "Median_imputation")) {
      # Simple methods
      imputed_full <- model_config$func(train, target_col, train_idx, test_idx)
    } else if(model_name %in% c("MVN_Amelia", "missForest")) {
      # Methods with feature_cols
      imputed_full <- model_config$func(train, target_col, feature_cols, train_idx, test_idx)
    } else {
      # Methods with additional parameters
      params <- model_config$params
      imputed_full <- do.call(model_config$func, 
                              c(list(data = train, target_col = target_col, 
                                     feature_cols = feature_cols, 
                                     train_idx = train_idx, pred_idx = test_idx),
                                params))
    }
    
    # Get predictions for test set
    pred_values <- imputed_full[test_idx]
    true_values <- train[test_idx, target_col]
    
    # Compute metrics
    metrics <- compute_metrics(true_values, pred_values)
    
    # Store results
    test_results <- rbind(test_results, data.frame(
      Model = model_name,
      MSE = metrics$MSE,
      MAE = metrics$MAE,
      RMSE = metrics$RMSE,
      MAPE = metrics$MAPE
    ))
    
    cat(" Done\n")
    
  }, error = function(e) {
    cat(" Failed:", e$message, "\n")
  })
}

cat("\nEvaluation complete!\n")

# ==================================================================================================
# Display Test Results
# ==================================================================================================

cat("\n====================================\n")
cat("TEST SET RESULTS\n")
cat("====================================\n")

if(nrow(test_results) > 0) {
  # Sort by MSE
  test_results <- test_results %>%
    arrange(MSE)
  
  print(test_results)
  
  # Save test results
  test_results_file <- paste0(DATASET_NAME, "_test_results.csv")
  write.csv(test_results, test_results_file, row.names = FALSE)
  cat("\nTest results saved to:", test_results_file, "\n")
} else {
  cat("No methods completed successfully\n")
  test_results_file <- NULL
}

# ==================================================================================================
# Apply All Methods to Actual Missing Values
# ==================================================================================================

cat("\n====================================\n")
cat("IMPUTING ACTUAL MISSING VALUES\n")
cat("====================================\n")

cat("Training on ALL", length(complete_idx), "complete cases\n")
cat("Predicting", length(missing_idx), "missing values\n\n")

# Store all imputations
all_imputations <- data.frame(
  id = train[missing_idx, id_col]
)
colnames(all_imputations)[1] <- id_col

# Apply each model to actual missing data
for(model_config in model_configs) {
  model_name <- model_config$name
  
  cat("Imputing with:", model_name, "...")
  
  tryCatch({
    # Train on all complete cases, predict missing cases
    if(model_name %in% c("Mean_imputation", "Median_imputation")) {
      # Simple methods
      imputed_full <- model_config$func(train, target_col, complete_idx, missing_idx)
    } else if(model_name %in% c("MVN_Amelia", "missForest")) {
      # Methods with feature_cols
      imputed_full <- model_config$func(train, target_col, feature_cols, complete_idx, missing_idx)
    } else {
      # Methods with additional parameters
      params <- model_config$params
      imputed_full <- do.call(model_config$func, 
                              c(list(data = train, target_col = target_col, 
                                     feature_cols = feature_cols, 
                                     train_idx = complete_idx, pred_idx = missing_idx),
                                params))
    }
    
    # Store imputations for missing IDs
    all_imputations[[model_name]] <- imputed_full[missing_idx]
    
    cat(" Done\n")
    
  }, error = function(e) {
    cat(" Failed:", e$message, "\n")
    # Store NAs if method failed
    all_imputations[[model_name]] <- rep(NA, length(missing_idx))
  })
}

cat("\nAll imputations completed!\n")

# Save all imputations
imputation_file <- paste0(DATASET_NAME, "_all_imputations.csv")
write.csv(all_imputations, imputation_file, row.names = FALSE)
cat("All imputations saved to:", imputation_file, "\n")

# ==================================================================================================
# Validation Set Check (SANITY CHECK ONLY)
# ==================================================================================================

cat("\n====================================\n")
cat("VALIDATION SET CHECK\n")
cat("====================================\n")
cat("NOTE: This is a sanity check only\n")
cat("Results below are NOT used for model selection or optimization\n")
cat("Use these to manually verify if predictions are reasonable\n\n")

# Calculate metrics for each method
validation_results <- data.frame()
validation_file <- NULL

if(nrow(validate_true_vals) > 0) {
  for(method_name in setdiff(colnames(all_imputations), id_col)) {
    
    # Get imputed values for this method
    method_imputations <- all_imputations[[method_name]]
    
    # Skip if method failed (all NAs)
    if(all(is.na(method_imputations))) {
      next
    }
    
    # Merge with validation true values
    method_df <- data.frame(
      id = all_imputations[[id_col]],
      imputed = method_imputations
    )
    colnames(method_df)[1] <- id_col
    
    validation_check <- method_df %>%
      inner_join(validate_true_vals, by = id_col)
    
    if(nrow(validation_check) > 0) {
      metrics <- compute_metrics(validation_check$true_target, validation_check$imputed)
      
      validation_results <- rbind(validation_results, data.frame(
        Model = method_name,
        MSE = metrics$MSE,
        MAE = metrics$MAE,
        RMSE = metrics$RMSE,
        MAPE = metrics$MAPE
      ))
    }
  }
  
  if(nrow(validation_results) > 0) {
    # Sort by MSE
    validation_results <- validation_results %>%
      arrange(MSE)
    
    cat("Validation Performance (all methods):\n")
    print(validation_results)
    
    # Save validation results
    validation_file <- paste0(DATASET_NAME, "_validation_results.csv")
    write.csv(validation_results, validation_file, row.names = FALSE)
    cat("\nValidation results saved to:", validation_file, "\n")
    
    cat("\nREMINDER: These validation results are for reference only.\n")
    cat("They were NOT used to select or optimize models.\n")
    cat("Compare with test results to check consistency.\n")
  } else {
    cat("No validation results computed (methods may have failed)\n")
  }
} else {
  cat("No matching validation values found\n")
}

# ==================================================================================================
# Final Summary
# ==================================================================================================

cat("\n====================================\n")
cat("SUMMARY\n")
cat("====================================\n")

# Create summary report
summary_report <- data.frame(
  Metric = c("Dataset", "Complete Cases", "Train Set", "Test Set", 
             "Missing Values Imputed", "N Methods Tested", "Test Split %"),
  Value = c(
    DATASET_NAME,
    length(complete_idx),
    length(train_idx),
    length(test_idx),
    length(missing_idx),
    length(model_configs),
    paste0(TEST_SPLIT * 100, "%")
  )
)

summary_file <- paste0(DATASET_NAME, "_summary_report.csv")
write.csv(summary_report, summary_file, row.names = FALSE)

cat("\n=== FILES SAVED ===\n")
if(!is.null(test_results_file)) {
  cat("-", test_results_file, "(test set performance - use this for model comparison)\n")
}
cat("-", imputation_file, "(all imputations from all methods)\n")
if(!is.null(validation_file)) {
  cat("-", validation_file, "(validation sanity check - NOT used for optimization)\n")
}
cat("-", summary_file, "(summary report)\n")

cat("\n=== ANALYSIS COMPLETE ===\n")
cat("\n=== CORRECT WORKFLOW ===\n")
cat("1. Separated complete cases (no NA) from missing cases (NA in target)\n")
cat("2. Split complete cases into train/test (", TEST_SPLIT*100, "% test)\n", sep="")
cat("3. Trained models on train set ONLY\n")
cat("4. Evaluated on test set (these are the performance metrics to use)\n")
cat("5. Retrained on ALL complete cases and predicted actual missing values\n")
cat("6. Checked validation set as sanity check (NOT used for model selection)\n")
cat("\n=== NO DATA LEAKAGE ===\n")
cat("- Models never saw validation data during training or selection\n")
cat("- Test metrics show real predictive performance\n")
cat("- Validation check confirms predictions are reasonable\n")
